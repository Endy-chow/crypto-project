{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7441a2",
   "metadata": {},
   "source": [
    "Transfer learning using cryptobert and roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f08c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig, TextClassificationPipeline\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e60cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20081cbe",
   "metadata": {},
   "source": [
    "## Step 1 - Importing a sample of some Bitcoin Tweet Data to begin analysing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f73c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20230308 - test data from twitter api, from 20220624 to 20220812 \n",
    "\n",
    "data = pd.read_csv('~/Code/giadapi/crypto/data/raw/tweet_24-6-22_to_23-9-22.csv')\n",
    "\n",
    "# 20230307 - test data from kaggle\n",
    "# data = pd.read_csv('~/code/giadapi/crypto/data/raw/bitcoin_tweets1000000.csv', nrows = 1000)\n",
    "# \"\\\\wsl.localhost\\Ubuntu\\home\\peter\\code\\giadapi\\crypto\\kaggle-tweets.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec789df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303cb8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e950bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run it if the dataset is from Twitter API\n",
    "# data['text'] = data[['tweet']]\n",
    "# data['date'] = data[['created_at']]\n",
    "\n",
    "#only run if the dataset is tweets_2021_reduced.csv\n",
    "data['date'] = data['datetime']\n",
    "for i in range(len(data)):\n",
    "    data['date'][i] = data['datetime'][i][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841439f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1549e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de89806",
   "metadata": {},
   "source": [
    "## Step 2 - Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have changed this to remove more information\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    for t in text.split(\" \"):\n",
    "        t = '' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = '' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e82154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dummy data\n",
    "data['process_text'] = data.text\n",
    "data['negative_bert'] = data.text\n",
    "data['neutral_bert'] = data.text\n",
    "data['positive_bert'] = data.text\n",
    "\n",
    "#use the preprocess_2 to clean the data\n",
    "data['process_text'] = data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95de5a6",
   "metadata": {},
   "source": [
    "## Step 3 - analyse the language and sentiments by pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyenv local crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d07bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2b - Use the bert model, with full scores\n",
    "MODEL_bert = f\"ElKulako/cryptobert\"\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(MODEL_bert)\n",
    "tokenizer_bert.model_max_length = 512 #solve the error: RuntimeError: The expanded size of the tensor (562) must match the existing size (514) at non-singleton dimension\n",
    "config_bert = AutoConfig.from_pretrained(MODEL_bert)\n",
    "\n",
    "\n",
    "# PT\n",
    "model_bert = AutoModelForSequenceClassification.from_pretrained(MODEL_bert)\n",
    "model_bert.config.max_position_embeddings = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63456b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_bert(sample_text):\n",
    "    encoded_input_bert = tokenizer_bert(sample_text, return_tensors='pt')\n",
    "    output_bert = model_bert(**encoded_input_bert)\n",
    "    scores_bert = output_bert[0][0].detach().numpy()\n",
    "    scores_bert = softmax(scores_bert) #1st score is negative, 2nd score is netural, 3rd score is positive\n",
    "    return scores_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379aabff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['process_text'].apply(scores_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfb746",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):    \n",
    "    data['negative_bert'][i] = data['text'][i][0]\n",
    "    data['neutral_bert'][i] = data['text'][i][1]\n",
    "    data['positive_bert'][i] = data['text'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['date', 'process_text', 'negative_bert', 'neutral_bert','positive_bert']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a164c5fd",
   "metadata": {},
   "source": [
    "## Step 4: Count the total number of labels/scores (positive, negative vs neutral) by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d67cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = pd.DataFrame(data.groupby(['date'])[['negative_bert', 'neutral_bert', 'positive_bert']].sum().fillna(0).reset_index())\n",
    "grouped_data = grouped_data.rename_axis(\"\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41232498",
   "metadata": {},
   "source": [
    "## Step 5 - Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = grouped_data['date'][0]\n",
    "# start_date_str = datetime.datetime.strftime(start_date, \"%Y-%m-%d\")\n",
    "start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2154ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = grouped_data['date'][len(grouped_data)-1]\n",
    "# end_date_str = datetime.datetime.strftime(end_date, \"%Y-%m-%d\")\n",
    "end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"{start_date}_{end_date}_twitter_comments.csv\"\n",
    "data.to_csv(f\"~/code/giadapi/crypto/data/processed/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"{start_date}_{end_date}_twitter_transferlearning.csv\"\n",
    "grouped_data.to_csv(f\"~/code/giadapi/crypto/data/processed/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f13bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
